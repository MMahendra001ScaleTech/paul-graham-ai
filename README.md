# PaulGrahamAI

PaulGrahamAI is a project designed to scrape, preprocess, and analyze Paul Graham's essays, Twitter posts, and Hacker News discussions to fine-tune an AI model. The project includes data collection, cleaning, training, and deployment components, making it a comprehensive pipeline for working with textual AI models.

---

## ğŸ“‚ Project Structure

```
PaulGrahamAI
â”‚â”€â”€ ğŸ“‚ data
â”‚   â”œâ”€â”€ essays.json          # âœ… Scraped essays
â”‚   â”œâ”€â”€ twitter.json         # â³ In Progress
â”‚   â”œâ”€â”€ hackernews.json      # ğŸš€ Future
â”‚
â”‚â”€â”€ ğŸ“‚ scraping
â”‚   â”œâ”€â”€ scrape_essays.py     # âœ… Done
â”‚   â”œâ”€â”€ scrape_twitter.py    # â³ Using snscrape
â”‚   â”œâ”€â”€ scrape_hn.py         # ğŸš€ Future
â”‚
â”‚â”€â”€ ğŸ“‚ preprocessing
â”‚   â”œâ”€â”€ clean_text.py        # Cleans raw data
â”‚   â”œâ”€â”€ format_data.py       # Converts to structured JSON
â”‚
â”‚â”€â”€ ğŸ“‚ training
â”‚   â”œâ”€â”€ fine_tune_gpt.py     # AI training script
â”‚   â”œâ”€â”€ model_evaluation.py  # Tests AI accuracy
â”‚
â”‚â”€â”€ ğŸ“‚ deployment
â”‚   â”œâ”€â”€ web_ui.py            # Flask / FastAPI web app
â”‚   â”œâ”€â”€ slack_bot.py         # Slack bot integration
â”‚
â”‚â”€â”€ requirements.txt         # All dependencies
â”‚â”€â”€ README.md                # Project documentation
```

---

## ğŸš€ Features

- **Scraping:** Extract essays, tweets, and Hacker News posts related to Paul Graham.
- **Preprocessing:** Clean and format the scraped text for AI training.
- **Fine-Tuning:** Train an AI model using GPT on the collected data.
- **Evaluation:** Assess the modelâ€™s performance on various NLP tasks.
- **Deployment:** Integrate the model into a web UI and Slack bot for easy interaction.

---

## ğŸ“¥ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/yourusername/PaulGrahamAI.git
cd PaulGrahamAI
```

### 2ï¸âƒ£ Install dependencies

Ensure you have Python 3.7+ installed, then run:

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Set up environment variables (if needed)

If any API keys are required (e.g., Slack, OpenAI API), create a `.env` file:

```bash
OPENAI_API_KEY=your_api_key
SLACK_BOT_TOKEN=your_token
```

---

## ğŸ”„ Usage

### Scraping Data

- **Essays:**

```bash
python scraping/scrape_essays.py
```

- **Twitter:**

```bash
python scraping/scrape_twitter.py
```

- **Hacker News:** _(Future implementation)_

```bash
python scraping/scrape_hn.py
```

### Preprocessing Data

```bash
python preprocessing/clean_text.py
python preprocessing/format_data.py
```

### Training the AI Model

```bash
python training/fine_tune_gpt.py
```

### Evaluating the Model

```bash
python training/model_evaluation.py
```

### Deploying the AI Model

- **Web UI (FastAPI)**

```bash
uvicorn deployment.web_ui:app --host 0.0.0.0 --port 8000
```

- **Slack Bot Integration**

```bash
python deployment/slack_bot.py
```

---

## ğŸ“š Dependencies

The project uses the following Python libraries:

```plaintext
snscrape
pandas
requests
beautifulsoup4
tqdm
transformers
fastapi
uvicorn
slack_sdk
```

Install them using:

```bash
pip install -r requirements.txt
```

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! To contribute:

1. Fork the repository
2. Create a new branch (`git checkout -b feature-branch`)
3. Commit your changes (`git commit -m 'Add new feature'`)
4. Push to the branch (`git push origin feature-branch`)
5. Create a Pull Request

---

## ğŸ“¬ Contact

For any questions or suggestions, reach out via email or create an issue in the repository.

Happy coding! ğŸš€
